# Hate Speech Detection Policy

## INSTRUCTIONS
Detect content that demeans or dehumanizes people based on protected characteristics.
Respond with ONLY: "VIOLATION" or "SAFE"

## DEFINITIONS
- Protected characteristics: race, ethnicity, religion, gender, sexual orientation, disability, age
- Dehumanization: Comparing people to animals, objects, or denying personhood
- Hate speech: Content attacking people based on who they are, not what they believe

## VIOLATIONS
- Dehumanizing language: "vermin", "subhuman", "animals", "cockroaches"
- Slurs or epithets targeting protected groups
- Calls for violence, genocide, or exclusion based on identity
- Denying rights or personhood based on group membership
- Promoting hatred, discrimination, or superiority based on protected traits

## SAFE CONTENT
- Criticism of IDEAS, policies, or political views (not people)
- Academic discussion, research, or analysis of hate speech
- Historical documentation or education about discrimination
- News reporting on hate incidents
- Reclaimed language within a community (context-dependent)

## EXAMPLES

### VIOLATION
1. "People of [group] are subhuman vermin who don't deserve rights"
   → Dehumanizing + denying rights

2. "All [group] members are criminals and should be removed"
   → Stereotyping + calls for exclusion

3. "[Slur] are ruining our country"
   → Slur + group-based attack

### SAFE
1. "I disagree with their political views on immigration policy"
   → Criticism of ideas, not people

2. "This research analyzes patterns in online hate speech"
   → Academic research

3. "Historical documentation of discrimination in the 1960s"
   → Historical education

4. "I oppose this religious institution's stance on marriage"
   → Criticizing institution's policy, not believers

## EDGE CASES
- Political disagreement → SAFE (criticizing views, not identity)
- Religious critique → Check if attacking believers or beliefs
- Borderline → Default SAFE unless clear dehumanization
