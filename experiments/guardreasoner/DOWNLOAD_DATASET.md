# Downloading GuardReasoner Training Dataset

**üéÅ CRITICAL DISCOVERY**: GuardReasoner's full training dataset (128K samples) is publicly available on HuggingFace!

---

## Quick Start

### 1. Install Dependencies
```bash
pip install datasets transformers
```

### 2. Download Dataset
```python
from datasets import load_dataset

# Download full training dataset
ds = load_dataset("yueliu1999/GuardReasonerTrain")

# Check what's included
print(ds)
# DatasetDict({
#     WildGuardTrainR: Dataset({
#         features: ['instruction', 'input', 'output', ...],
#         num_rows: 86800
#     }),
#     AegisTrainR: Dataset({...}),
#     BeaverTailsTrainR: Dataset({...}),
#     ToxicChatTrainR: Dataset({...})
# })
```

### 3. Examine Format
```python
# Look at first sample
sample = ds['WildGuardTrainR'][0]
print(sample)

# Expected format:
# {
#   'instruction': 'You are a classifier... [3-task template]',
#   'input': 'Human user:\n{prompt}\n\nAI assistant:\n{response}\n\n',
#   'output': 'Answers:\nRequest: unharmful\nCompletion: compliance\nResponse: unharmful\n',
#   'prompt_harm_label': 'unharmful',
#   'response_refusal_label': 'compliance',
#   'response_harm_label': 'unharmful'
# }
```

---

## Dataset Details

### Splits & Sizes

| Split | Samples | Source | Purpose |
|-------|---------|--------|---------|
| WildGuardTrainR | 86,800 | AllenAI WildGuard | Adversarial safety |
| AegisTrainR | 10,800 | Nvidia Aegis | Policy violations |
| BeaverTailsTrainR | 27,200 | PKU BeaverTails | Harmful behaviors |
| ToxicChatTrainR | 2,800 | ToxicChat | Toxic language |
| **TOTAL** | **127,600** | 4 datasets | Comprehensive |

### What's Included

‚úÖ **Prompt + Response pairs** (fixes your missing response issue!)
‚úÖ **3-task labels**:
  - Task 1: Prompt harmfulness (harmful/unharmful)
  - Task 2: Refusal detection (refusal/compliance)
  - Task 3: Response harmfulness (harmful/unharmful)
‚úÖ **Reasoning traces** (generated by GPT-4o)
‚úÖ **Diverse sources** (4 major safety benchmarks)
‚úÖ **Ready for R-SFT** (pre-formatted for training)

---

## Convert to Your Format

### Option 1: Use As-Is (Recommended)
```python
# The dataset is already in ChatML format - use directly!
from datasets import load_dataset

ds = load_dataset("yueliu1999/GuardReasonerTrain")

# Combine all splits
full_dataset = concatenate_datasets([
    ds['WildGuardTrainR'],
    ds['AegisTrainR'],
    ds['BeaverTailsTrainR'],
    ds['ToxicChatTrainR']
])

print(f"Total samples: {len(full_dataset)}")  # 127,600

# Save locally
full_dataset.to_json("guardreasoner_train_full.json")
```

### Option 2: Convert to Unsloth Format
```python
# If using Unsloth for LoRA training
def convert_to_unsloth(sample):
    """Convert GuardReasoner format to Unsloth ChatML."""
    return {
        "text": f"{sample['instruction']}\n{sample['input']}\n{sample['output']}"
    }

# Apply conversion
unsloth_dataset = full_dataset.map(convert_to_unsloth)

# Save for training
unsloth_dataset.to_json("guardreasoner_train_unsloth.json")
```

### Option 3: Use Specific Splits
```python
# If you want to start small (e.g., just WildGuard)
wildguard_only = ds['WildGuardTrainR']
print(f"WildGuard samples: {len(wildguard_only)}")  # 86,800

# Save
wildguard_only.to_json("guardreasoner_wildguard_only.json")
```

---

## Training with This Dataset

### Update Your Experiment 18 Config

**OLD (your current setup)**:
```python
dataset_path = "data/guardreasoner_train_chatml.json"  # 11K samples
num_samples = 11,396
```

**NEW (with GuardReasoner data)**:
```python
dataset_path = "data/guardreasoner_train_full.json"  # 128K samples
num_samples = 127,600
```

### Training Time Estimate

**With 128K samples** (11√ó more than current):
- Epoch 1: ~88 hours (vs 8 hours with 11K)
- 3 epochs: ~264 hours (~11 days)
- 5 epochs (paper): ~440 hours (~18 days)

**Recommendation**: Start with WildGuardTrainR only (87K samples):
- Epoch 1: ~60 hours (~2.5 days)
- 3 epochs: ~180 hours (~7.5 days)
- Still 7√ó more data than your current setup

### Hardware Considerations

**Your current setup** (11K samples, 4-bit LoRA):
- GPU memory: ~12-16GB
- Training time: 8 hours/epoch

**With 128K samples**:
- GPU memory: ~12-16GB (same, batch size stays small)
- Training time: ~88 hours/epoch (11√ó longer)
- Disk space: ~5-10GB for dataset

**Optimization tips**:
1. Use fewer gradient accumulation steps (64 ‚Üí 32)
2. Start with WildGuardTrainR only (87K ‚Üí 60 hours/epoch)
3. Train on nigel.birs.ca in screen session
4. Monitor first 1000 steps to estimate total time

---

## Comparison: Your Dataset vs GuardReasoner

| Feature | Your Current | GuardReasoner | Improvement |
|---------|--------------|---------------|-------------|
| Samples | 11,396 | 127,600 | **11√ó more** |
| Sources | 2 (harmful/harmless) | 4 (diverse) | **2√ó more diverse** |
| Tasks | 1 (harmful/safe) | 3 (prompt/refusal/response) | **3√ó more tasks** |
| Response included | ‚ùå No | ‚úÖ Yes | **Critical fix** |
| Reasoning traces | ‚úÖ Yes | ‚úÖ Yes (GPT-4o) | **Higher quality** |
| Format | ChatML | ChatML | **Same format** |
| License | MIT | MIT | **Same (free)** |

---

## Recommended Next Steps

### Option A: Full Restart (Clean Comparison) ‚≠ê RECOMMENDED
```bash
cd ~/wizard101/experiments/guardreasoner

# 1. Download their full dataset
python3 << 'EOF'
from datasets import load_dataset, concatenate_datasets

ds = load_dataset("yueliu1999/GuardReasonerTrain")
full = concatenate_datasets([
    ds['WildGuardTrainR'],
    ds['AegisTrainR'],
    ds['BeaverTailsTrainR'],
    ds['ToxicChatTrainR']
])
full.to_json("data/guardreasoner_train_full.json")
print(f"Downloaded {len(full)} samples")
EOF

# 2. Start fresh R-SFT training (Experiment 19)
python3 scripts/experiment_19_rsft_full_dataset.py

# 3. Compare results to paper
python3 evaluate_exp_19.py
```

### Option B: Continue Current Model (Faster Validation)
```bash
# 1. Download just WildGuardTrainR (87K samples)
python3 << 'EOF'
from datasets import load_dataset
ds = load_dataset("yueliu1999/GuardReasonerTrain")
ds['WildGuardTrainR'].to_json("data/guardreasoner_wildguard.json")
EOF

# 2. Continue training from epoch 1 checkpoint
python3 scripts/experiment_18_continue_wildguard.py

# 3. Compare epoch 1 (11K) vs epoch 2 (87K additional)
```

### Option C: Staged Approach (Most Practical)
```bash
# Stage 1: Finish current R-SFT (1 epoch ‚Üí 3 epochs)
python3 scripts/experiment_18_continue.py  # 2 more epochs with 11K

# Stage 2: Evaluate and compare
python3 evaluate_exp_18_final.py

# Stage 3: Train full model with GuardReasoner data
python3 scripts/experiment_19_full_guardreasoner.py

# Stage 4: Compare your 11K vs their 128K
```

---

## Expected Performance Improvement

### Current (Exp 18, 11K samples, 1 epoch)
```
Accuracy: 59.0%
Harmful F1: 0.713
Safe F1: 0.480
```

### After Full GuardReasoner Training (128K, 3 epochs)
```
Accuracy: 75-80% (expected)
Harmful F1: 0.80-0.85
Safe F1: 0.75-0.80
Multi-task: Can now evaluate all 3 tasks!
```

### Paper Results (128K, 5 epochs, 8B model)
```
Accuracy: ~84%
Prompt Harmful F1: 0.87
Response Refusal F1: 0.81
Response Harmful F1: 0.82
```

**Gap explanation**:
- Your 3B model: 75-80% (expected)
- Paper 8B model: 84%
- **Difference: 4-9%** due to model size (acceptable!)

---

## Quick Download Script

Save this as `download_guardreasoner.py`:

```python
#!/usr/bin/env python3
"""Download GuardReasoner training dataset."""

from datasets import load_dataset, concatenate_datasets
import json

print("Downloading GuardReasonerTrain dataset...")
ds = load_dataset("yueliu1999/GuardReasonerTrain")

# Show splits
for split_name, split_data in ds.items():
    print(f"  {split_name}: {len(split_data)} samples")

# Combine all splits
print("\nCombining splits...")
full_dataset = concatenate_datasets([
    ds['WildGuardTrainR'],
    ds['AegisTrainR'],
    ds['BeaverTailsTrainR'],
    ds['ToxicChatTrainR']
])

print(f"Total: {len(full_dataset)} samples")

# Save
output_path = "data/guardreasoner_train_full.json"
print(f"\nSaving to {output_path}...")
full_dataset.to_json(output_path)

# Show sample
print("\nFirst sample:")
print(json.dumps(full_dataset[0], indent=2))

print("\n‚úÖ Done! Dataset ready for training.")
```

Run it:
```bash
python3 download_guardreasoner.py
```

---

## Bottom Line

**üéâ This is a game-changer!**

You can now:
1. ‚úÖ Use their exact 128K training dataset
2. ‚úÖ Train with 3-task format (prompt/refusal/response)
3. ‚úÖ Include response input (fixing critical gap)
4. ‚úÖ Compare directly to paper's results
5. ‚úÖ Skip months of dataset creation work

**Next action**: Download the dataset and restart training with their data!

**Timeline**:
- Download: 5 minutes
- Training (3 epochs): 7-11 days
- Evaluation: 1 hour
- **Total**: ~2 weeks to full replication

**Expected outcome**: 75-80% accuracy (vs paper's 84% with 8B model)
