# GuardReasoner Evaluation Setup - COMPLETE ‚úÖ

## Your Pipeline & Targets

### Paper Baseline (GuardReasoner-8B)
- **Overall F1**: ~84% (beats GPT-4o by 5.74%)
- **WildGuard Test**: 1,554 samples

### Your Implementation Progress

| Metric | Current (1 epoch) | Target (3 epochs) | Target (Full) |
|--------|-------------------|-------------------|---------------|
| Overall Accuracy | 59% | 70-75% | 80-85% |
| **Harmful F1** | **0.713** | **0.80** | **0.85** |
| **Safe F1** | **0.480** | **0.70** | **0.80** |
| Dataset Size | 11K | 11K | 128K+ |

---

## Standard Evaluation Scripts (Ready to Use)

### 1. **eval_standard.py** ‚≠ê PRIMARY
**Use this for model comparison**

```bash
# Baseline evaluation
python eval_standard.py

# Evaluate your trained model (edit MODEL_PATH first)
# Line 28: MODEL_PATH = "./models/exp_18_rsft_lora"
python eval_standard.py
```

**Features:**
- ‚úÖ 100 samples from `combined_test.json` (your standard)
- ‚úÖ Reports **Harmful F1** and **Safe F1** (matches Exp 18 format)
- ‚úÖ Confusion matrix
- ‚úÖ MLX 4-bit support (fast!)
- ‚úÖ Output matches your evaluation reports exactly

**Speed:**
- MLX 4-bit: ~13 min (100 samples)
- PyTorch float16: ~40 min (100 samples)

---

### 2. **eval_mlx_wildguard.py** (Full Benchmark)
**Use for paper comparison**

```bash
# Full WildGuard benchmark (1554 samples)
# Edit NUM_SAMPLES = None for full test
python eval_mlx_wildguard.py
```

**Features:**
- ‚úÖ 1,554 samples (official WildGuard benchmark)
- ‚úÖ Compare to paper (F1 ~0.78-0.82 for 3B)
- ‚úÖ MLX 4-bit optimized

**Speed:**
- MLX 4-bit: ~3 hours (1554 samples)
- PyTorch float16: ~10 hours

---

### 3. **eval_fast_mac.py** (Quick Testing)
**Use for rapid iteration**

```bash
python eval_fast_mac.py
```

**Features:**
- ‚úÖ 50 samples (quick test)
- ‚úÖ Fast results (~40 min)
- ‚úÖ Good for debugging

---

## MLX Quantized Models (Ready)

### GuardReasoner-3B (4-bit)
**Location:** `./mlx_models/gr3b-q4/`
**Size:** 1.7GB (down from 6GB)
**Speed:** 2-3x faster than float16

### Benefits:
- ‚ö° Fast inference (~7-13 samples/sec)
- üíæ 4x less memory
- üçé Native Apple Silicon optimization

---

## How to Evaluate Your Models

### Step 1: Standard 100-sample Test (Matches Exp 18)

```bash
cd ~/development/wizard101/experiments/guardreasoner

# 1. Edit eval_standard.py line 28:
# MODEL_PATH = "./models/exp_18_rsft_lora"  # Your model
# or
# MODEL_PATH = "./models/exp_19_hsdpo_toy_lora"  # DPO model

# 2. Run evaluation
python eval_standard.py > results_exp18.log 2>&1

# 3. View results
tail -40 results_exp18.log
cat results_standard.json | jq '.metrics'
```

**Expected Output:**
```
Per-Class Metrics:

Class      Precision    Recall       F1 Score
--------------------------------------------------
Harmful       58.6%      91.1%        0.713
Safe          90.0%      32.7%        0.480

Comparison to Targets:
  Harmful F1: 0.713 (Target: 0.75-0.80)
  Safe F1: 0.480 (Target: 0.60-0.70)
```

---

### Step 2: Full WildGuard Benchmark (Optional)

```bash
# Edit eval_mlx_wildguard.py:
# MODEL_PATH = "./models/exp_18_rsft_lora"
# NUM_SAMPLES = None  # Full test

python eval_mlx_wildguard.py > results_wildguard.log 2>&1 &

# Monitor (takes ~3 hours)
tail -f results_wildguard.log
```

---

## Comparison Table

| Script | Samples | Time (MLX) | Time (PyTorch) | Purpose |
|--------|---------|------------|----------------|---------|
| **eval_standard.py** | 100 | 13 min | 40 min | **Model comparison** ‚≠ê |
| eval_mlx_wildguard.py | 1554 | 3 hours | 10 hours | Paper benchmark |
| eval_fast_mac.py | 50 | 7 min | 20 min | Quick testing |

---

## Your Workflow

### After Training Epoch 2:
```bash
# 1. Evaluate on standard test
python eval_standard.py  # MODEL_PATH = epoch 2 checkpoint

# 2. Compare to epoch 1
# Epoch 1: Harmful F1 = 0.713, Safe F1 = 0.480
# Epoch 2: Harmful F1 = ?, Safe F1 = ?

# 3. Decide if training is improving
```

### After Training Epoch 3:
```bash
# 1. Standard test
python eval_standard.py

# 2. Check if targets met:
# ‚úÖ Harmful F1 ‚â• 0.80?
# ‚úÖ Safe F1 ‚â• 0.70?
# ‚úÖ Accuracy ‚â• 70%?

# 3. If yes ‚Üí Proceed to HS-DPO
# 4. If no ‚Üí Train more epochs
```

---

## Files Created Today

### Evaluation Scripts
```
eval_standard.py           ‚≠ê PRIMARY - Your standard eval
eval_mlx_wildguard.py      Full WildGuard benchmark
eval_fast_mac.py           Quick 50-sample test
eval_mlx_quantized.py      MLX quantized (general)
eval_official_format.py    Official format (slow)
```

### Model Conversion
```
convert_to_mlx.py          Convert models to MLX 4-bit
mlx_models/gr3b-q4/        GuardReasoner-3B (quantized)
```

### Documentation
```
MLX_EVALUATION_GUIDE.md    Complete MLX setup guide
EVALUATION_SETUP_COMPLETE.md  This file
```

### Results (from today's tests)
```
results_fast_mac.json      50 samples, F1 = 0.958 (easy test set)
results_standard.json      100 samples (when you run it)
results_wildguard_mlx.json 1554 samples (when you run it)
```

---

## Next Steps

### Immediate:
1. ‚úÖ Run `python eval_standard.py` to get baseline on your 100-sample test
2. ‚úÖ Compare to Exp 18 results
3. ‚úÖ Verify MLX is faster than PyTorch

### When You Train More:
1. Epoch 2 complete ‚Üí Run eval_standard.py
2. Epoch 3 complete ‚Üí Run eval_standard.py
3. Compare: Harmful F1 improving? Safe F1 improving?

### Future:
1. Convert your trained models to MLX for fast eval:
   ```bash
   python convert_to_mlx.py \
     --model ./models/exp_18_rsft_lora \
     --quantize q4 \
     --output ./mlx_models/exp18-q4
   ```

2. Run full WildGuard benchmark (3 hours)

---

## Quick Reference

**Evaluate baseline (MLX 4-bit):**
```bash
python eval_standard.py
```

**Evaluate your model:**
```bash
# Edit line 28: MODEL_PATH = "./models/exp_18_rsft_lora"
python eval_standard.py
```

**Monitor running eval:**
```bash
tail -f results_standard.log
```

**Check results:**
```bash
cat results_standard.json | jq '.metrics.harmful.f1, .metrics.safe.f1'
```

---

## Success! üéâ

**You now have:**
- ‚úÖ Fast MLX 4-bit inference (2-3x speedup)
- ‚úÖ Standard evaluation matching your Exp 18 format
- ‚úÖ Full WildGuard benchmark capability
- ‚úÖ Reproducible workflow for model comparison

**Key Metrics to Track:**
- **Harmful F1** (primary) - Target: 0.80 (epoch 3), 0.85 (full)
- **Safe F1** (secondary) - Target: 0.70 (epoch 3), 0.80 (full)

**Ready to evaluate your trained models!**
