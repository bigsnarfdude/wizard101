# GuardReasoner Quick Start Summary

## What We Have Now (2025-11-18)

### âœ… Completed: SFT Training (Experiment 18)
- **Model**: Llama-3.2-3B-Instruct with LoRA adapter
- **Training**: 1 epoch R-SFT on 11,396 samples (8 hours)
- **Location**: `~/wizard101/experiments/guardreasoner/models/exp_18_rsft_lora/`
- **HuggingFace**: vincentoh/guardreasoner-llama3.2-3b-lora-1epoch
- **Purpose**: Foundation model with reasoning traces

### ðŸ”„ In Progress: Quick Evaluation
- **Script**: `evaluate_exp_18_quick.py`
- **Dataset**: 100 samples from combined_test.json
- **Runtime**: ~10 minutes (6 seconds/sample)
- **Status**: 25% complete (as of 14:18 UTC)
- **Checks**:
  - Model loads correctly âœ…
  - Adapter applies successfully âœ…
  - Generates reasoning traces âœ…

### ðŸ“‹ Next: RL Training Pipeline

## Dataset Information

### Training Data (`guardreasoner_train_chatml.json`)
- **Size**: 11,396 samples
- **Format**: ChatML with reasoning traces
- **Sources**:
  - Harmful Behaviors: Adversarial safety prompts
  - Harmless Alpaca: Benign instructions
- **Structure**:
```json
{
  "conversations": [
    {"from": "human", "value": "[prompt]"},
    {"from": "gpt", "value": "[reasoning]\nLabel: harmful/safe"}
  ]
}
```

### Test Data (`combined_test.json`)
- **Size**: 200 samples
- **Used for**: Quick evaluation (100 samples randomly sampled)
- **Format**:
```json
{
  "text": "prompt text here",
  "label": "harmless" or "harmful",
  "category": "harmless" or "harmful"
}
```

**Note**: Script normalizes `"harmless"` â†’ `"safe"` for consistency

## Model Architecture

### How It Works
1. **Base**: Llama-3.2-3B-Instruct (4-bit quantized)
2. **Adapter**: LoRA with rank=16
   - Targets: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
   - Alpha: 16, Dropout: 0
3. **Inference**: Base model + LoRA weights
4. **Output**: Reasoning trace + safety label

### Evaluation Process
```python
# 1. Load base model (4-bit)
base = AutoModelForCausalLM.from_pretrained("unsloth/Llama-3.2-3B-Instruct", load_in_4bit=True)

# 2. Load LoRA adapter
model = PeftModel.from_pretrained(base, "exp_18_rsft_lora")

# 3. Create prompt (ChatML format)
prompt = "<|im_start|>user\nAnalyze this content for safety:\n...<|im_end|>\n<|im_start|>assistant\n..."

# 4. Generate reasoning + label
output = model.generate(prompt, max_tokens=512, temperature=0.7)

# 5. Parse output
reasoning, label = parse_response(output)
```

## Why RL Training Next?

### The Paper's Finding (IMPORTANT)
**GRPO (Group Relative Policy Optimization) FAILED** âŒ
- Tried on difficult samples starting from SFT model
- Result: -1.1% average F1 score
- Problem: "Difficulty improving safety guards distilled from strong teacher"

### Our Different Approach
**DPO on Reasoning Quality** (not just accuracy) âœ…

**Key Innovation**: Optimize reasoning traces, not just labels

#### Traditional RL (Paper's Mistake)
```
Reward = 1 if correct_label else 0
Problem: No room to improve when SFT already matches teacher accuracy
```

#### Our RL Approach
```
Reward = 0.3 * label_correctness +
         0.3 * reasoning_coherence +
         0.2 * reasoning_relevance +
         0.2 * reasoning_specificity

Advantage: Can improve reasoning quality even when accuracy plateaus
```

### RL Pipeline (Toy â†’ Full)

**Phase 1: Toy Experiment** (1-2 days)
```
1. Generate 1k samples Ã— 4 completions = 4k generations
2. Score each: reasoning quality + label correctness
3. Create preference pairs: (good reasoning, bad reasoning)
4. DPO training: 1 epoch (~2 hours)
5. Evaluate: Better reasoning? Similar accuracy?
```

**Phase 2: Full RL** (3-4 days, if toy works)
```
1. Generate 10k samples Ã— 4 completions = 40k generations
2. Score all with automated judge
3. Create 10k preference pairs
4. DPO training: 2-3 epochs (~24 hours)
5. Full evaluation: WildGuard test set (1,554 samples)
```

## Success Criteria

### Toy Experiment (Exp 19)
- âœ… Accuracy: Within Â±2% of SFT baseline
- âœ… Reasoning quality: +10% improvement (automated score)
- âœ… Human preference: 60% prefer DPO reasoning

### Full Experiment (Exp 20)
- âœ… Accuracy: 59-61% on WildGuard (baseline: 57.5%)
- âœ… Reasoning quality: +10-15% improvement
- âœ… Human preference: 65% prefer DPO reasoning

## File Locations

### On nigel.birs.ca
```
~/wizard101/experiments/guardreasoner/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ guardreasoner_train_chatml.json (11,396 samples)
â”‚   â””â”€â”€ combined_test.json (200 samples)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ exp_18_rsft_lora/ (1-epoch SFT model)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ exp_18_quick_eval_v2.log (current evaluation)
â”œâ”€â”€ results/
â”‚   â””â”€â”€ exp_18_quick_eval.json (will be created)
â””â”€â”€ scripts/
    â””â”€â”€ evaluate_exp_18_quick.py
```

### On MacBook (local)
```
~/development/wizard101/experiments/guardreasoner/
â”œâ”€â”€ RL_TRAINING_PLAN.md (RL strategy document)
â”œâ”€â”€ RESEARCH_VALIDATION.md (paper replication plan)
â”œâ”€â”€ EVALUATION_PLAN.md (decision framework)
â””â”€â”€ QUICK_START_SUMMARY.md (this file)
```

### On HuggingFace
```
vincentoh/guardreasoner-llama3.2-3b-lora-1epoch
â”œâ”€â”€ adapter_model.safetensors (93MB - LoRA weights)
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ tokenizer files
â””â”€â”€ README.md (model card)
```

## Next Steps

### Immediate (Today)
1. âœ… Wait for quick eval to finish (~10 min remaining)
2. âœ… Check if model generates coherent reasoning
3. âœ… Verify accuracy >50% (go/no-go for RL)

### Tomorrow
1. ðŸ“‹ Create `create_rl_preferences.py` script
2. ðŸ“‹ Generate toy preference dataset (1k pairs)
3. ðŸ“‹ Start toy DPO training

### This Week
1. ðŸ“‹ Evaluate toy DPO results
2. ðŸ“‹ Decision: Scale to full RL or iterate?
3. ðŸ“‹ If good: Create full preference dataset

## Key Files to Create

### For RL Training
- `create_rl_preferences.py` - Generate preference pairs from multiple completions
- `judge_reasoning_quality.py` - Automated scoring of reasoning traces
- `train_exp_19_dpo_toy.py` - Toy DPO experiment (1k pairs)
- `train_exp_20_dpo_full.py` - Full DPO experiment (10k pairs)

### For Evaluation
- `evaluate_exp_19_dpo.py` - Compare SFT vs DPO
- `human_eval_tool.py` - Interface for human preference study
- `analyze_reasoning_improvements.py` - What did RL improve?

## Commands Reference

### Monitor Evaluation
```bash
# Check progress
ssh user@server "tail -f ~/wizard101/experiments/guardreasoner/logs/exp_18_quick_eval_v2.log"

# Check results
ssh user@server "cat ~/wizard101/experiments/guardreasoner/results/exp_18_quick_eval.json"
```

### Upload Scripts
```bash
# From local machine
scp experiments/guardreasoner/script.py user@server:~/wizard101/experiments/guardreasoner/
```

### Run Training
```bash
# On nigel
cd ~/wizard101/experiments/guardreasoner
source venv/bin/activate
python script.py > logs/output.log 2>&1 &
```

## Research Questions

1. **Does 1-epoch SFT work?** (testing now)
2. **Can RL improve reasoning quality?** (next phase)
3. **Is DPO better than GRPO for this task?** (hypothesis: yes)
4. **What's the sweet spot: accuracy vs reasoning quality?**

---

**Last Updated**: 2025-11-18 14:19 UTC
**Status**: Evaluation running (25% complete)
**Next Milestone**: Evaluate reasoning quality when eval finishes
