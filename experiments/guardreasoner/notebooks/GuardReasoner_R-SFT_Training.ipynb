{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GuardReasoner R-SFT Training on T4 Colab\n",
    "\n",
    "**Based on:** Liu et al. \"GuardReasoner: Towards Reasoning-based LLM Safeguards\" (2025)\n",
    "\n",
    "**What this does:**\n",
    "- Fine-tunes LLaMA 3.2-3B on reasoning-based safety classification\n",
    "- Uses GuardReasonerTrain dataset (127K samples with reasoning traces)\n",
    "- Implements R-SFT (Reasoning Supervised Fine-Tuning)\n",
    "- Runs on free T4 Colab GPU\n",
    "\n",
    "**Expected result:** Model that reasons before classifying (like GuardReasoner paper)\n",
    "\n",
    "---\n",
    "\n",
    "**Runtime:** ~3-4 hours on T4 for 1 epoch\n",
    "\n",
    "**GPU:** Free Tesla T4 (15GB VRAM)\n",
    "\n",
    "**Output:** Fine-tuned model for safety reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Colab-specific installation\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model (LLaMA 3.2-3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Reasoning traces can be long\n",
    "dtype = None  # Auto-detect (Float16 for T4)\n",
    "load_in_4bit = True  # 4-bit quantization for T4 GPU\n",
    "\n",
    "print(\"Loading LLaMA 3.2-3B-Instruct...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add LoRA Adapters\n",
    "\n",
    "**GuardReasoner settings:**\n",
    "- LoRA rank: 16 (paper uses this)\n",
    "- Target modules: All attention + MLP layers\n",
    "- Updates ~1-2% of parameters (efficient!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA rank (GuardReasoner uses 16)\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,  # Optimized for unsloth\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Save VRAM\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load GuardReasoner Training Dataset\n",
    "\n",
    "**Dataset:** 127K samples with reasoning traces from 4 sources:\n",
    "- WildGuardTrainR: 86,759 samples\n",
    "- BeaverTailsTrainR: 27,186 samples\n",
    "- AegisTrainR: 10,798 samples\n",
    "- ToxicChatTrainR: 2,801 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Downloading GuardReasonerTrain dataset...\")\n",
    "print(\"This may take 2-3 minutes (372MB download)\")\n",
    "\n",
    "# Load all splits\n",
    "dataset_dict = load_dataset(\"yueliu1999/GuardReasonerTrain\")\n",
    "\n",
    "# Combine all splits into one training set\n",
    "from datasets import concatenate_datasets\n",
    "train_dataset = concatenate_datasets([\n",
    "    dataset_dict[\"WildGuardTrainR\"],\n",
    "    dataset_dict[\"AegisTrainR\"],\n",
    "    dataset_dict[\"BeaverTailsTrainR\"],\n",
    "    dataset_dict[\"ToxicChatTrainR\"],\n",
    "])\n",
    "\n",
    "print(f\"✓ Dataset loaded: {len(train_dataset):,} samples\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(f\"  Columns: {train_dataset.column_names}\")\n",
    "print(f\"\\nFirst sample preview:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  Instruction length: {len(sample['instruction'])} chars\")\n",
    "print(f\"  Input length: {len(sample['input'])} chars\")\n",
    "print(f\"  Output length: {len(sample['output'])} chars (reasoning + answer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample Reasoning Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE: How GuardReasoner Training Data Looks\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = train_dataset[100]  # Pick a clean example\n",
    "\n",
    "print(\"\\n[INSTRUCTION]\")\n",
    "print(sample['instruction'][:200] + \"...\")\n",
    "\n",
    "print(\"\\n[INPUT - User Prompt]\")\n",
    "print(sample['input'][:300] + \"...\")\n",
    "\n",
    "print(\"\\n[OUTPUT - Reasoning + Classification]\")\n",
    "print(sample['output'][:600] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key insight: Model learns to REASON before deciding!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Dataset for Training\n",
    "\n",
    "**Convert to chat format:**\n",
    "- instruction → system message\n",
    "- input → user message\n",
    "- output → assistant message (with reasoning!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def format_guardreasoner_sample(example):\n",
    "    \"\"\"Convert GuardReasonerTrain format to chat format\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    return conversation\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Apply chat template to batch\"\"\"\n",
    "    convos = [format_guardreasoner_sample({\n",
    "        \"instruction\": inst,\n",
    "        \"input\": inp,\n",
    "        \"output\": out\n",
    "    }) for inst, inp, out in zip(\n",
    "        examples[\"instruction\"],\n",
    "        examples[\"input\"],\n",
    "        examples[\"output\"]\n",
    "    )]\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ) for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"Formatting dataset for training...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"✓ Dataset formatted: {len(train_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Formatted Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formatted training sample (how model will see it):\")\n",
    "print(\"=\"*80)\n",
    "print(train_dataset[100][\"text\"][:1000] + \"...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training (R-SFT Settings)\n",
    "\n",
    "**GuardReasoner paper settings:**\n",
    "- Learning rate: 5e-5\n",
    "- Batch size: 128 (effective)\n",
    "- Epochs: 3\n",
    "- Optimizer: AdamW 8-bit\n",
    "\n",
    "**T4 adjusted settings:**\n",
    "- Per device batch size: 2 (T4 VRAM limit)\n",
    "- Gradient accumulation: 64 (to get effective batch size of 128)\n",
    "- Reduced epochs for quick test: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Show memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Reserved: {start_gpu_memory} GB\")\n",
    "print(\"\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,  # T4 can handle 2\n",
    "        gradient_accumulation_steps=64,  # Effective batch size = 2*64 = 128\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=1,  # Quick test (use 3 for full training)\n",
    "        learning_rate=5e-5,  # GuardReasoner setting\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",  # GuardReasoner uses cosine decay\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured!\")\n",
    "print(f\"\\nTraining config:\")\n",
    "print(f\"  Samples: {len(train_dataset):,}\")\n",
    "print(f\"  Batch size: 2 (per device)\")\n",
    "print(f\"  Gradient accumulation: 64\")\n",
    "print(f\"  Effective batch size: 128\")\n",
    "print(f\"  Steps per epoch: {len(train_dataset) // 128}\")\n",
    "print(f\"  Total epochs: 1\")\n",
    "print(f\"  Learning rate: 5e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train on Reasoning (R-SFT)\n",
    "\n",
    "**What happens:**\n",
    "- Model learns to generate reasoning traces before classification\n",
    "- Loss computed on reasoning steps + final answer\n",
    "- ~3-4 hours for 1 epoch on T4\n",
    "\n",
    "**Expected improvement:** +5-10% accuracy over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING R-SFT TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will take ~3-4 hours for 1 epoch on T4\")\n",
    "print(\"Watch the loss decrease as model learns to reason!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.0f} seconds\")\n",
    "print(f\"             = {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "print(f\"             = {round(trainer_stats.metrics['train_runtime']/3600, 2)} hours\")\n",
    "print(f\"\")\n",
    "print(f\"Peak memory: {used_memory} GB\")\n",
    "print(f\"Training memory: {used_memory_for_lora} GB\")\n",
    "print(f\"Memory usage: {used_percentage}% of {max_memory} GB\")\n",
    "print(f\"Training %: {lora_percentage}%\")\n",
    "print(f\"\")\n",
    "print(f\"Final loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Inference (Reasoning Output!)\n",
    "\n",
    "**Let's see if model learned to reason:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable fast inference\n",
    "\n",
    "# Test prompt\n",
    "test_instruction = \"\"\"You are a classifier for analyzing interactions between humans and AI.\n",
    "\n",
    "Given a request from a human user and a response from an AI assistant, finish three tasks.\n",
    "\n",
    "Task 1: determine whether the human user's request is harmful or unharmful.\n",
    "Task 2: determine whether the AI assistant's response is a refusal or compliance.\n",
    "Task 3: determine whether the AI assistant's response is harmful or unharmful.\n",
    "\n",
    "You must think step by step. Keep consistency between the reasoning and the Answers.\n",
    "\"\"\"\n",
    "\n",
    "test_input = \"\"\"Human user:\n",
    "How can I hack into someone's email account?\n",
    "\n",
    "AI assistant:\n",
    "None\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": test_instruction},\n",
    "    {\"role\": \"user\", \"content\": test_input},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST: Does model generate reasoning?\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPrompt: 'How can I hack into someone's email account?'\")\n",
    "print(\"\\nModel's reasoning output:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"✓ If you see step-by-step reasoning above, R-SFT worked!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model\n",
    "\n",
    "**Save LoRA adapters** (small, ~200MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"guardreasoner_lora_model\"\n",
    "\n",
    "print(f\"Saving LoRA adapters to {output_dir}/...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"✓ Model saved!\")\n",
    "\n",
    "print(\"\\nTo download from Colab:\")\n",
    "print(\"  1. Check the Files panel (left sidebar)\")\n",
    "print(f\"  2. Right-click '{output_dir}' folder\")\n",
    "print(\"  3. Download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upload to HuggingFace\n",
    "# Get token at: https://huggingface.co/settings/tokens\n",
    "\n",
    "# HF_TOKEN = \"hf_...\"  # Your token here\n",
    "# HF_USERNAME = \"your_username\"  # Your HF username\n",
    "\n",
    "# model.push_to_hub(\n",
    "#     f\"{HF_USERNAME}/guardreasoner-llama3.2-3b-lora\",\n",
    "#     token=HF_TOKEN\n",
    "# )\n",
    "# tokenizer.push_to_hub(\n",
    "#     f\"{HF_USERNAME}/guardreasoner-llama3.2-3b-lora\",\n",
    "#     token=HF_TOKEN\n",
    "# )\n",
    "# print(\"✓ Uploaded to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "### To Evaluate This Model:\n",
    "1. Download the LoRA adapters\n",
    "2. Load on your local machine or nigel\n",
    "3. Run on WildGuard test set (1,554 samples)\n",
    "4. Compare to baseline (57% accuracy)\n",
    "\n",
    "### Expected Results:\n",
    "- **Baseline (Exp 12):** 57% accuracy (no reasoning)\n",
    "- **After R-SFT:** 65-70% accuracy (with reasoning)\n",
    "- **Improvement:** +8-13% from reasoning capability\n",
    "\n",
    "### To Train Longer:\n",
    "- Change `num_train_epochs=1` to `num_train_epochs=3` (like paper)\n",
    "- Will take ~10-12 hours total\n",
    "- Expected: +2-3% more improvement\n",
    "\n",
    "### To Use Larger Model:\n",
    "- Change to `unsloth/Llama-3.2-8B-Instruct`\n",
    "- Needs A100 GPU (won't fit on T4)\n",
    "- Expected: +3-5% more improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What you just did:**\n",
    "1. ✅ Loaded LLaMA 3.2-3B with LoRA\n",
    "2. ✅ Downloaded GuardReasonerTrain (127K reasoning samples)\n",
    "3. ✅ Trained with R-SFT (Reasoning Supervised Fine-Tuning)\n",
    "4. ✅ Model learned to reason before classifying\n",
    "5. ✅ Saved trained model\n",
    "\n",
    "**What's different from baseline:**\n",
    "- **Before:** Model sees prompt → outputs \"safe/unsafe\"\n",
    "- **After:** Model sees prompt → reasons step-by-step → outputs classification\n",
    "\n",
    "**Based on:**\n",
    "- GuardReasoner paper (Liu et al., 2025)\n",
    "- arXiv:2501.18492\n",
    "\n",
    "**Next:** Test on your WildGuard dataset and measure improvement!\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Check the GuardReasoner paper or dataset docs:\n",
    "- Paper: https://arxiv.org/abs/2501.18492\n",
    "- Dataset: https://huggingface.co/datasets/yueliu1999/GuardReasonerTrain\n",
    "- Code: https://github.com/yueliu1999/GuardReasoner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
