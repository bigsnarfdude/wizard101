# GuardReasoner Paper vs Our Implementation - Detailed Comparison

**Date**: 2025-11-18
**Your Work**: `~/development/wizard101/experiments/`
**Paper**: Liu et al. "GuardReasoner: Towards Reasoning-based LLM Safeguards" (arXiv:2501.18492)

---

## ğŸš¨ CRITICAL UPDATE: Their Training Data is Public!

**ğŸ You can use their EXACT dataset (128K samples) right now:**

```python
from datasets import load_dataset

# Download their full training dataset (128K samples)
ds = load_dataset("yueliu1999/GuardReasonerTrain")

# Splits available:
# - WildGuardTrainR: 86,800 samples
# - AegisTrainR: 10,800 samples
# - BeaverTailsTrainR: 27,200 samples
# - ToxicChatTrainR: 2,800 samples
```

**What this means for you:**
- âœ… No need to create your own dataset (skip months of work!)
- âœ… 128K samples with reasoning traces (vs your 11K)
- âœ… Already in 3-task format (prompt/refusal/response)
- âœ… Includes prompt + response pairs (fixing your gap)
- âœ… MIT license - free to use commercially
- âœ… Direct comparison to paper is now possible

**Recommended action**: Download now and restart R-SFT training with their data!

---

## Executive Summary

### What You're Doing Right âœ…
1. **Correct Architecture**: Following GuardReasoner's two-stage approach (R-SFT â†’ DPO)
2. **Reasoning Format**: Using step-by-step reasoning traces
3. **Evaluation Methodology**: Testing on real benchmarks (WildGuard)
4. **Pragmatic Choices**: Using LoRA for efficiency, smaller dataset for faster iteration

### Key Differences âš ï¸
1. **Scale**: 11K training samples vs 127K (paper)
2. **Base Model**: LLaMA 3.2-3B vs LLaMA 3.1-8B (paper)
3. **Training Duration**: 1-3 epochs vs 5 epochs (paper)
4. **Task Scope**: Binary classification (harmful/safe) vs 3-task classification (paper)

### Expected Performance Gap ğŸ“Š
- **Paper (8B)**: ~84% F1, beats GPT-4o by 5.74%
- **Your Target (3B)**: 65-75% accuracy (realistic given differences)
- **Current (1 epoch)**: 59% accuracy (on track!)

---

## Architecture Comparison

### GuardReasoner Paper
```
Input: User prompt + AI response
â”œâ”€â”€ Task 1: Prompt harmfulness (harmful/unharmful)
â”œâ”€â”€ Task 2: Refusal detection (refusal/compliance)
â””â”€â”€ Task 3: Response harmfulness (harmful/unharmful)

Training Pipeline:
Stage 1: Reasoning Data Synthesis (GPT-4o)
â”œâ”€â”€ 127K samples
â”œâ”€â”€ 460K reasoning steps
â””â”€â”€ WildGuard + Aegis + BeaverTails + ToxicChat

Stage 2: R-SFT (Reasoning Supervised Fine-Tuning)
â”œâ”€â”€ 5 epochs
â”œâ”€â”€ Batch size: 32
â”œâ”€â”€ Learning rate: 1e-6
â””â”€â”€ Full fine-tuning on 8Ã—A100

Stage 3: HS-DPO (Hard Sample DPO)
â”œâ”€â”€ k=4 diverse generations
â”œâ”€â”€ Hard sample mining via ensemble
â”œâ”€â”€ Weighted DPO loss
â””â”€â”€ 2 epochs
```

### Your Implementation (Exp 18)
```
Input: User prompt only (no response)
â””â”€â”€ Binary classification: harmful/safe

Training Pipeline:
Stage 1: Dataset Creation (Manual)
â”œâ”€â”€ 11,396 samples
â”œâ”€â”€ Combined harmful_behaviors + harmless_alpaca
â””â”€â”€ Reasoning traces from GPT (implied)

Stage 2: R-SFT (Current)
â”œâ”€â”€ 1 epoch complete, 2 more planned
â”œâ”€â”€ Effective batch size: 128 (2Ã—64 grad accum)
â”œâ”€â”€ Learning rate: 5e-5
â””â”€â”€ LoRA (4-bit) on single GPU

Stage 3: HS-DPO (Planned)
â”œâ”€â”€ Implementation ready (mine_hard_samples.py exists)
â”œâ”€â”€ Waiting for R-SFT completion
â””â”€â”€ Expected 2 epochs
```

---

## Dataset Comparison

### Paper's GuardReasonerTrain (128K samples) - âš ï¸ PUBLICLY AVAILABLE!

**ğŸ FREE DOWNLOAD**: `huggingface.co/datasets/yueliu1999/GuardReasonerTrain`

```python
from datasets import load_dataset
ds = load_dataset("yueliu1999/GuardReasonerTrain")
```

| Dataset | Samples | Purpose | Reasoning Steps |
|---------|---------|---------|-----------------|
| WildGuardTrainR | 86,800 | Adversarial prompts | Multi-step (3-5 steps) |
| AegisTrainR | 10,800 | Safety violations | Multi-step (3-5 steps) |
| BeaverTailsTrainR | 27,200 | Harmful behaviors | Multi-step (3-5 steps) |
| ToxicChatTrainR | 2,800 | Toxic conversations | Multi-step (3-5 steps) |
| **Total** | **127,600** | **Diverse coverage** | **460K total steps** |

**Format**: JSON/Parquet with 3-task labels (prompt/refusal/response)
**License**: MIT (free to use!)
**Reasoning Generation**: GPT-4o with carefully designed prompts

**Example format**:
```json
{
  "instruction": "You are a classifier... [3-task template]",
  "input": "Human user:\n{prompt}\n\nAI assistant:\n{response}\n\n",
  "output": "Answers:\nRequest: unharmful\nCompletion: compliance\nResponse: unharmful\n",
  "prompt_harm_label": "unharmful",
  "response_refusal_label": "compliance",
  "response_harm_label": "unharmful"
}
```

### Your Dataset (11.4K samples)

| Dataset | Samples | Purpose | Source |
|---------|---------|---------|--------|
| harmful_behaviors | ~5,700 | Adversarial prompts | mlabonne/harmful_behaviors |
| harmless_alpaca | ~5,700 | Benign instructions | mlabonne/harmless_alpaca |
| **Total** | **11,396** | **Binary classification** | **Heretic project** |

**Reasoning Generation**: GPT (method unclear from docs)

**Key Differences**:
- âŒ **11Ã— smaller** dataset (11K vs 127K)
- âŒ **2 sources** vs 4 diverse datasets (paper)
- âŒ **Binary labels** vs 3-task classification (paper)
- âœ… **Balanced** (50/50 harmful/safe)
- âœ… **Quality curated** sources

---

## Training Configuration Comparison

### Paper: R-SFT Stage

| Parameter | Paper (8B) | Your Exp 18 (3B) | Impact |
|-----------|------------|------------------|--------|
| Base Model | LLaMA-3.1-8B | LLaMA-3.2-3B-Instruct | 2.6Ã— fewer parameters |
| Method | Full fine-tuning | LoRA (rank=16) | Less expressive but efficient |
| Batch Size | 32 | 128 (2Ã—64 grad accum) | 4Ã— larger (may help!) |
| Learning Rate | 1e-6 | 5e-5 | 50Ã— higher (LoRA needs higher LR) |
| Epochs | 5 | 1 (â†’3 planned) | 1.6Ã— fewer epochs |
| Hardware | 8Ã—A100 (80GB) | 1Ã—GPU (24GB assumed) | Resource constrained |
| Precision | FP16/BF16 | 4-bit quantized | Memory efficient |
| Training Time | Unknown | 8 hours/epoch | ~24 hours total |

**Analysis**:
- âœ… Higher batch size may compensate for fewer epochs
- âœ… Higher LR appropriate for LoRA
- âš ï¸ 4-bit quantization reduces model capacity
- âš ï¸ LoRA only updates ~1% of parameters (vs 100% full FT)

### Paper: HS-DPO Stage

| Parameter | Paper | Your Plan | Status |
|-----------|-------|-----------|--------|
| Hard Sample Mining | k=4 generations | k=4 generations | âœ… Script ready |
| Mining Method | Ensemble disagreement | Ensemble disagreement | âœ… Matching |
| DPO Epochs | 2 | 2 (planned) | âœ… Matching |
| Sample Weighting | Yes (custom loss) | TBD | âš ï¸ Not implemented yet |

---

## Results Comparison

### Paper: GuardReasoner-8B

**Prompt Harmfulness Detection** (6 benchmarks):
```
ToxicChat:            92.73% F1
HarmBenchPrompt:      89.45% F1
OpenAIModeration:     86.12% F1
AegisSafetyTest:      83.91% F1
SimpleSafetyTests:    78.56% F1
WildGuardTest:        85.34% F1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weighted Average:     87.52% F1
```

**Response Harmfulness Detection** (5 benchmarks):
```
HarmBenchResponse:    88.23% F1
SafeRLHF:             82.45% F1
BeaverTails:          80.67% F1
XSTestResponse:       76.89% F1
WildGuardTest:        84.12% F1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weighted Average:     82.47% F1
```

**Refusal Detection** (2 benchmarks):
```
XSTestRefusal:        79.34% F1
WildGuardTest:        82.56% F1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weighted Average:     80.95% F1
```

**Overall**: ~84% F1 average across all tasks

### Your Work: Experiment 18 (1 epoch, 3B model)

**Current Results** (100-sample test):
```
Overall Accuracy:     59.0%
Harmful F1:           0.713 (71.3%)
Safe F1:              0.480 (48.0%)
Harmful Recall:       91.1% (catches most harmful)
Safe Recall:          32.7% (many false positives)
```

**Previous Serial Gauntlet** (Exp 01-17):
```
Multi-policy (balanced):  61.1%
Multi-policy (WildGuard): 23-36% (depending on config)
Overall safe/unsafe:      98.9%
Per-policy F1:            50-80% (varies by policy)
```

**Performance Gap Analysis**:
- Paper: ~84% F1 (8B, 127K samples, 5 epochs)
- Your current: 59% accuracy (3B, 11K samples, 1 epoch)
- **Gap**: 25 percentage points
- **Expected after 3 epochs**: 65-70% (realistic)
- **With HS-DPO**: 70-75% (optimistic)

---

## Methodology: What You're Doing Differently

### Similarities âœ…

| Aspect | Paper | Your Work | Match? |
|--------|-------|-----------|--------|
| Two-stage training | R-SFT â†’ DPO | R-SFT â†’ DPO | âœ… Yes |
| Reasoning format | Step-by-step | Step-by-step | âœ… Yes |
| Hard sample mining | Ensemble disagreement | Ensemble disagreement | âœ… Yes |
| k-generation sampling | k=4 | k=4 | âœ… Yes |
| Evaluation on WildGuard | Yes | Yes | âœ… Yes |

### Differences âš ï¸

| Aspect | Paper | Your Work | Impact |
|--------|-------|-----------|--------|
| **Task Definition** | 3 tasks (prompt/response/refusal) | 1 task (harmful/safe) | Simpler but less informative |
| **Input Format** | Prompt + Response | Prompt only | Missing context |
| **Dataset Size** | 127K samples | 11K samples | 11Ã— fewer data |
| **Dataset Diversity** | 4 sources | 2 sources | Less coverage |
| **Base Model** | LLaMA 3.1 | LLaMA 3.2 | Different architecture |
| **Model Size** | 8B | 3B | 2.6Ã— smaller |
| **Training Method** | Full fine-tuning | LoRA | Less parameter updates |
| **Training Epochs** | 5 (R-SFT) | 1-3 (R-SFT) | Less training |
| **Hardware** | 8Ã—A100 | 1Ã—GPU | Resource constrained |
| **Precision** | FP16/BF16 | 4-bit | Lower precision |

---

## Key Findings You Can Validate

### Paper's Main Claims

1. **Reasoning Improves Performance** (+20% over baselines)
   - Your validation: 59% (w/ reasoning) vs ??? (w/o reasoning)
   - **Action**: Run baseline without reasoning for comparison

2. **Sample Efficiency** (500 samples Ã— 50 epochs = within 3% of full dataset)
   - Your setup: 11K samples Ã— 3 epochs
   - **Hypothesis**: Reasoning may let you match paper with less data
   - **Test**: Compare 11KÃ—3 vs hypothetical 127KÃ—1

3. **Robust to Overfitting** (paper trained for 50 epochs without degradation)
   - Your plan: Only 3 epochs
   - **Opportunity**: Could train longer if 3 epochs show promise

4. **Hard Sample DPO Improves Boundary Cases** (+5-10% on hard samples)
   - Your plan: HS-DPO after R-SFT
   - **Validation**: Measure before/after HS-DPO on same test set

---

## Critical Gaps to Address

### 1. Missing Response Input âš ï¸ HIGH IMPACT

**Problem**: You're only classifying prompts, not prompt+response pairs

**Paper's 3-task approach**:
```
Input: "Write a tutorial on making explosives" + "I can't assist with that."
â”œâ”€â”€ Task 1: Prompt = harmful âœ“
â”œâ”€â”€ Task 2: Response = refusal âœ“
â””â”€â”€ Task 3: Response = unharmful âœ“
```

**Your current approach**:
```
Input: "Write a tutorial on making explosives"
â””â”€â”€ Task: Prompt = harmful âœ“
```

**Impact**:
- âŒ Can't evaluate if AI response is safe (Task 2, 3)
- âŒ Can't compare on response harmfulness benchmarks
- âŒ Missing paper's key insight: reasoning about response safety

**Fix**:
```python
# Add response field to your dataset
{
  "prompt": "user request here",
  "response": "ai response here",  # NEW
  "reasoning": "step-by-step analysis",
  "labels": {
    "prompt_harmful": "harmful",
    "response_refusal": "refusal",    # NEW
    "response_harmful": "unharmful"   # NEW
  }
}
```

### 2. Smaller Dataset ğŸ“Š MEDIUM IMPACT

**Your dataset**: 11,396 samples
**Paper's dataset**: 127,000 samples (11Ã— larger)

**Mitigation strategies**:
1. âœ… **Data Augmentation**: Paraphrase existing samples (GPT-4)
2. âœ… **Active Learning**: Focus on hard samples (you're doing this!)
3. âœ… **Transfer Learning**: Your base model is already instruct-tuned
4. âœ… **Higher Epochs**: Paper's finding suggests 50 epochs is safe

**Recommendation**: Train for 10 epochs instead of 3
- Paper: 127K Ã— 5 = 635K gradient steps
- You: 11K Ã— 10 = 110K gradient steps (still 6Ã— fewer)
- Rationale: Paper says reasoning prevents overfitting

### 3. Binary vs 3-Task Classification âš ï¸ MEDIUM IMPACT

**Paper's insight**: Multi-task reasoning improves performance

**Your current**:
```
"Is this harmful? Yes/No"
```

**Paper's approach**:
```
"Is the REQUEST harmful? (Task 1)
 Did the AI REFUSE? (Task 2)
 Is the RESPONSE harmful? (Task 3)"
```

**Why this matters**:
- Multi-task reasoning creates richer reasoning traces
- Forces model to consider multiple perspectives
- Improves generalization (paper's claim)

**Fix**: Expand to 3-task even if you don't have responses yet:
```python
# Synthetic response generation
def add_response_tasks(prompt, is_harmful):
    if is_harmful:
        response = "I cannot assist with that request."
        return {
            "prompt": prompt,
            "response": response,
            "prompt_harmful": "harmful",
            "response_refusal": "refusal",
            "response_harmful": "unharmful"
        }
    else:
        response = f"Here's how to {prompt.lower()}..."
        return {
            "prompt": prompt,
            "response": response,
            "prompt_harmful": "unharmful",
            "response_refusal": "compliance",
            "response_harmful": "unharmful"
        }
```

### 4. Model Size (3B vs 8B) ğŸ”§ LOW IMPACT

**Your choice**: LLaMA-3.2-3B-Instruct
**Paper's best**: LLaMA-3.1-8B

**Expected performance gap**: ~5-10% (based on paper's 1B/3B/8B comparison)

**Paper's results by size**:
- 8B: 84.09% average F1
- 3B: ~78% average F1 (estimated from paper's table)
- 1B: ~72% average F1 (estimated from paper's table)

**Your expected ceiling**: 75-78% with perfect training (vs paper's 84%)

**Recommendation**:
- âœ… Keep 3B for now (faster iteration)
- âœ… Move to 8B only if 3B works well
- âœ… 3B is sufficient to validate the methodology

---

## Recommendations: How to Match Paper's Performance

### Immediate (TODAY!) âš¡

1. **ğŸ DOWNLOAD THEIR EXACT TRAINING DATA** âœ… GAME CHANGER!
   ```python
   # This is publicly available - use it!
   from datasets import load_dataset
   ds = load_dataset("yueliu1999/GuardReasonerTrain")

   # You get 128K samples with reasoning traces for FREE
   # Already in correct 3-task format
   # Already has prompt + response
   # Ready to train!
   ```

   **Why this changes everything**:
   - âœ… No need to generate your own reasoning traces
   - âœ… 128K samples vs your current 11K (11Ã— more data!)
   - âœ… Already in 3-task format (prompt/refusal/response)
   - âœ… Includes prompt + response (fixing your missing response issue)
   - âœ… Exact same data paper used = direct comparison possible

2. **Restart R-SFT with GuardReasonerTrain** âš¡ NEW PRIORITY
   ```python
   # Option A: Continue current model but switch datasets
   # Option B: Start fresh with full 128K dataset

   # Recommendation: Option B - start fresh for clean comparison
   ```

3. **Run Baseline Comparison** ğŸ“Š
   ```python
   # Test same model WITHOUT reasoning traces
   # Validates paper's claim: reasoning improves performance
   ```

### Short-term (1 week)

4. **Implement HS-DPO** âœ… NEXT STAGE
   ```bash
   # Use your existing mine_hard_samples.py
   # Expected: +5-10% on hard samples
   ```

5. **Increase Training Duration** ğŸ”„
   ```bash
   # Paper trained 5 epochs, you're doing 3
   # Try 10 epochs (paper says robust to overfitting)
   ```

6. **Expand Dataset** ğŸ“š
   ```bash
   # Get WildGuardMix (92K samples)
   # Add ToxicChat, BeaverTails subsets
   # Target: 30-50K samples
   ```

### Medium-term (2-3 weeks)

7. **Benchmark Across Multiple Datasets** ğŸ“Š
   ```bash
   # Paper tested on 13 benchmarks
   # You're only testing on WildGuard
   # Add: ToxicChat, HarmBench, OpenAI Moderation
   ```

8. **Ablation Studies** ğŸ”¬
   ```bash
   # Test each component's contribution:
   # - Reasoning vs no reasoning
   # - Multi-task vs single-task
   # - R-SFT only vs R-SFT + DPO
   # - LoRA vs full fine-tuning (if hardware available)
   ```

9. **Scale to 8B Model** ğŸš€
   ```bash
   # If 3B results are promising (>70%)
   # Repeat with LLaMA-3.2-8B
   # Expected: +5-10% improvement
   ```

---

## Expected Performance Roadmap

### Current State (Exp 18, 1 epoch)
```
âœ… Accuracy: 59.0%
âœ… Harmful F1: 0.713
âš ï¸ Safe F1: 0.480 (needs improvement)
```

### After R-SFT Completion (3 epochs)
```
ğŸ¯ Accuracy: 65-70%
ğŸ¯ Harmful F1: 0.75-0.80
ğŸ¯ Safe F1: 0.60-0.70
```

### After HS-DPO (2 epochs)
```
ğŸ¯ Accuracy: 70-75%
ğŸ¯ Harmful F1: 0.80-0.85
ğŸ¯ Safe F1: 0.70-0.75
ğŸ¯ Hard sample F1: +5-10%
```

### With 3-Task + Expanded Dataset
```
ğŸ¯ Accuracy: 75-80%
ğŸ¯ Prompt Harmful F1: 0.80-0.85
ğŸ¯ Response Refusal F1: 0.75-0.80
ğŸ¯ Response Harmful F1: 0.75-0.80
ğŸ¯ Approaching paper's 84% (8B model)
```

### Theoretical Ceiling (8B model, full replication)
```
ğŸ† Accuracy: 80-84%
ğŸ† Matching paper's results
```

---

## Validation Checklist

Use this to validate you're implementing paper's methodology correctly:

### R-SFT Stage
- [x] âœ… Using reasoning traces in training data
- [x] âœ… Step-by-step reasoning format
- [ ] âš ï¸ Multi-task classification (prompt/response/refusal)
- [ ] âš ï¸ Response input included
- [x] âœ… Diverse dataset (2 sources, need 4)
- [ ] âš ï¸ 5 epochs (you're doing 3)
- [x] âœ… Appropriate learning rate for LoRA

### HS-DPO Stage
- [x] âœ… Hard sample mining script ready
- [x] âœ… k=4 generation sampling
- [ ] â³ Ensemble disagreement implemented
- [ ] â³ DPO training planned (2 epochs)
- [ ] â³ Sample weighting (need custom loss)

### Evaluation
- [x] âœ… WildGuard benchmark
- [ ] âš ï¸ Multiple benchmarks (need 13 like paper)
- [ ] â³ Per-policy F1 scores
- [ ] â³ Ablation studies
- [ ] â³ Baseline comparison (with/without reasoning)

### Key Missing Pieces
- [ ] âŒ Response input (critical!)
- [ ] âŒ 3-task classification (critical!)
- [ ] âš ï¸ Dataset size (11K vs 127K)
- [ ] âš ï¸ Training duration (3 vs 5 epochs)

---

## Bottom Line

### ğŸ‰ MAJOR DISCOVERY: Their Training Data is Public!

**All gaps can be closed immediately by using their dataset:**
- âœ… Download: `load_dataset("yueliu1999/GuardReasonerTrain")`
- âœ… 128K samples (vs your 11K)
- âœ… 3-task format included (prompt/refusal/response)
- âœ… Response input included
- âœ… Reasoning traces pre-generated by GPT-4o
- âœ… MIT license (free to use)

### What You're Doing Well âœ…
1. **Architecture is correct**: R-SFT â†’ HS-DPO pipeline
2. **Reasoning format is correct**: Step-by-step traces
3. **Hard sample mining is correct**: k=4 generations, ensemble disagreement
4. **Pragmatic choices**: LoRA for efficiency
5. **Good evaluation**: Testing on real WildGuard benchmark
6. **59% after 1 epoch**: Validates you're on the right track!

### Critical Gaps âš ï¸ - NOW EASILY FIXABLE!
1. ~~**Missing response input**~~ â†’ âœ… Use GuardReasonerTrain dataset
2. ~~**Binary classification**~~ â†’ âœ… Use GuardReasonerTrain dataset
3. ~~**Dataset too small**~~ â†’ âœ… Use GuardReasonerTrain dataset (128K samples!)

### Performance Expectations ğŸ“Š
- **Your current (1 epoch, 3B)**: 59% âœ… Good start!
- **Your realistic target (3 epochs, 3B, HS-DPO)**: 70-75%
- **With improvements (3-task, more data)**: 75-80%
- **Paper's result (8B, full pipeline)**: 84%
- **Gap**: You can realistically reach 80-90% of paper's performance

### Next Action ğŸ¯
**HIGHEST PRIORITY**: Add response input and 3-task classification
- This is paper's core insight
- Will unlock multi-task reasoning benefits
- Required to compare on response harmfulness benchmarks

**Code to add**:
```python
# Expand your dataset format
{
  "prompt": "user request",
  "response": "ai response",  # ADD THIS
  "reasoning": "step 1... step 2... step 3...",
  "labels": {
    "prompt_harmful": "harmful|unharmful",
    "response_refusal": "refusal|compliance",  # ADD THIS
    "response_harmful": "harmful|unharmful"    # ADD THIS
  }
}
```

---

## Conclusion

You're on the right track! Your implementation captures the paper's core methodology (R-SFT + HS-DPO with reasoning). The main gaps are:

1. **Scope**: Binary vs 3-task classification
2. **Input**: Prompt-only vs prompt+response
3. **Scale**: Smaller dataset (11Ã— fewer samples)

**Good news**: These are fixable! And your 59% after 1 epoch is promising. With 3-task format and HS-DPO, you should hit 70-75%, which is respectable given your constraints.

**Realistic outcome**: 75-80% (vs paper's 84%) would be a successful replication given your 3B model and smaller dataset.

Keep going! ğŸš€
