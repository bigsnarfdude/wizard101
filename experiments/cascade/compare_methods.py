#!/usr/bin/env python3
"""
Compare All Cascade Methods

Test different configurations to find optimal approach:
1. L0 Only (fastest)
2. L0 + Multi-Reviewer
3. L0 + L1 + L2 Gauntlet (full cascade)

Usage:
    python compare_methods.py
"""

import json
import time
from pathlib import Path

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import requests
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

# Model paths
L0_MODEL = "./models/l0_bouncer"
OLLAMA_URL = "http://localhost:11434/api/generate"
SAFEGUARD_MODEL = "gpt-oss-safeguard"

# Thresholds
L0_THRESHOLD = 0.65


class CascadeComparison:
    """Compare different cascade methods."""

    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load L0
        print("Loading L0 Bouncer...")
        self.l0_tokenizer = AutoTokenizer.from_pretrained(L0_MODEL)
        self.l0_model = AutoModelForSequenceClassification.from_pretrained(L0_MODEL)
        self.l0_model.to(self.device)
        self.l0_model.eval()
        print(f"  L0 loaded on {self.device}")

    def run_l0(self, text):
        """Run L0 bouncer."""
        start = time.perf_counter()

        inputs = self.l0_tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=256,
        ).to(self.device)

        with torch.no_grad():
            outputs = self.l0_model(**inputs)
            probs = F.softmax(outputs.logits, dim=-1)

        latency = (time.perf_counter() - start) * 1000

        safe_prob = probs[0][0].item()
        harmful_prob = probs[0][1].item()

        return {
            "label": "safe" if safe_prob > harmful_prob else "harmful",
            "confidence": max(safe_prob, harmful_prob),
            "latency_ms": latency,
        }

    def run_safeguard(self, text):
        """Run gpt-oss-safeguard."""
        start = time.perf_counter()

        prompt = f"""Analyze: "{text}"
Is this SAFE or HARMFUL? One word:"""

        try:
            response = requests.post(
                OLLAMA_URL,
                json={"model": SAFEGUARD_MODEL, "prompt": prompt, "stream": False},
                timeout=30,
            )
            answer = response.json().get("response", "").upper()
            label = "harmful" if "HARMFUL" in answer else "safe"
        except:
            label = "safe"

        return {
            "label": label,
            "latency_ms": (time.perf_counter() - start) * 1000,
        }

    def run_heuristic(self, text):
        """Simple keyword heuristic."""
        harmful_kw = ["hack", "malware", "bomb", "weapon", "kill", "steal", "password", "exploit", "attack", "drug"]
        matches = sum(1 for kw in harmful_kw if kw in text.lower())
        return "harmful" if matches >= 2 else "safe"

    # Method 1: L0 Only
    def method_l0_only(self, text):
        """Just L0 bouncer."""
        result = self.run_l0(text)
        return {
            "label": result["label"],
            "latency_ms": result["latency_ms"],
            "method": "L0_only",
        }

    # Method 2: L0 + Safeguard
    def method_l0_safeguard(self, text):
        """L0 + escalate to safeguard."""
        l0 = self.run_l0(text)

        if l0["confidence"] >= L0_THRESHOLD and l0["label"] == "safe":
            return {
                "label": "safe",
                "latency_ms": l0["latency_ms"],
                "method": "L0_safeguard",
                "escalated": False,
            }

        # Escalate
        safeguard = self.run_safeguard(text)
        return {
            "label": safeguard["label"],
            "latency_ms": l0["latency_ms"] + safeguard["latency_ms"],
            "method": "L0_safeguard",
            "escalated": True,
        }

    # Method 3: L0 + Heuristic + Safeguard (voting)
    def method_l0_vote(self, text):
        """L0 + heuristic + safeguard vote."""
        l0 = self.run_l0(text)

        if l0["confidence"] >= L0_THRESHOLD and l0["label"] == "safe":
            return {
                "label": "safe",
                "latency_ms": l0["latency_ms"],
                "method": "L0_vote",
                "escalated": False,
            }

        # Vote with heuristic + safeguard
        heuristic = self.run_heuristic(text)
        safeguard = self.run_safeguard(text)

        # Any harmful = harmful
        if heuristic == "harmful" or safeguard["label"] == "harmful":
            label = "harmful"
        else:
            label = "safe"

        return {
            "label": label,
            "latency_ms": l0["latency_ms"] + safeguard["latency_ms"],
            "method": "L0_vote",
            "escalated": True,
        }

    # Method 4: L0 + 3 Expert Gauntlet (reduced from 6)
    def method_l0_gauntlet3(self, text):
        """L0 + 3 key experts."""
        l0 = self.run_l0(text)

        if l0["confidence"] >= L0_THRESHOLD and l0["label"] == "safe":
            return {
                "label": "safe",
                "latency_ms": l0["latency_ms"],
                "method": "L0_gauntlet3",
                "escalated": False,
            }

        # Run 3 key experts
        experts = [
            ("violence", "Detect violence, weapons, threats. SAFE or HARMFUL:"),
            ("illegal", "Detect hacking, malware, drugs. SAFE or HARMFUL:"),
            ("privacy", "Detect privacy violations, doxxing. SAFE or HARMFUL:"),
        ]

        total_latency = l0["latency_ms"]
        votes = {"safe": 0, "harmful": 0}

        for name, prompt_suffix in experts:
            start = time.perf_counter()
            try:
                response = requests.post(
                    OLLAMA_URL,
                    json={
                        "model": SAFEGUARD_MODEL,
                        "prompt": f'{prompt_suffix}\n"{text}"',
                        "stream": False,
                    },
                    timeout=30,
                )
                answer = response.json().get("response", "").upper()
                if "HARMFUL" in answer:
                    votes["harmful"] += 1
                else:
                    votes["safe"] += 1
            except:
                votes["safe"] += 1

            total_latency += (time.perf_counter() - start) * 1000

        label = "harmful" if votes["harmful"] >= 1 else "safe"

        return {
            "label": label,
            "latency_ms": total_latency,
            "method": "L0_gauntlet3",
            "escalated": True,
            "votes": votes,
        }

    def compare_all(self, test_data, n_samples=50):
        """Compare all methods on test data."""
        if len(test_data) > n_samples:
            import random
            test_data = random.sample(test_data, n_samples)

        methods = [
            ("L0_only", self.method_l0_only),
            ("L0_safeguard", self.method_l0_safeguard),
            ("L0_vote", self.method_l0_vote),
            ("L0_gauntlet3", self.method_l0_gauntlet3),
        ]

        results = {name: [] for name, _ in methods}

        print(f"\nTesting {len(test_data)} samples across {len(methods)} methods...")

        for i, item in enumerate(test_data):
            text = item.get("prompt", item.get("text", ""))
            expected = item.get("label", "").lower()
            if expected == "harmless":
                expected = "safe"

            for name, method in methods:
                result = method(text)
                result["expected"] = expected
                result["correct"] = result["label"] == expected
                results[name].append(result)

            if (i + 1) % 10 == 0:
                print(f"  Processed {i+1}/{len(test_data)}")

        return results


def main():
    print("=" * 60)
    print("CASCADE METHOD COMPARISON")
    print("=" * 60)

    # Load test data
    data_path = Path("../combined_test.json")
    if not data_path.exists():
        data_path = Path("data/combined_test.json")

    if not data_path.exists():
        print("‚ùå Test data not found")
        return

    with open(data_path) as f:
        test_data = json.load(f)

    # Run comparison
    cascade = CascadeComparison()
    results = cascade.compare_all(test_data, n_samples=50)

    # Print results
    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)

    print(f"\n{'Method':<20} {'Accuracy':>10} {'Escalated':>10} {'Mean ms':>10} {'P50 ms':>10}")
    print("-" * 60)

    best_accuracy = 0
    best_method = None

    for name, data in results.items():
        accuracy = sum(1 for r in data if r["correct"]) / len(data)
        escalated = sum(1 for r in data if r.get("escalated", False)) / len(data)
        latencies = [r["latency_ms"] for r in data]
        mean_lat = np.mean(latencies)
        p50_lat = np.percentile(latencies, 50)

        print(f"{name:<20} {accuracy*100:>9.1f}% {escalated*100:>9.1f}% {mean_lat:>9.1f} {p50_lat:>9.1f}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_method = name

    print("-" * 60)
    print(f"\nüèÜ BEST METHOD: {best_method} ({best_accuracy*100:.1f}% accuracy)")

    # Detailed best method analysis
    best_data = results[best_method]

    # Confusion matrix
    tp = sum(1 for r in best_data if r["expected"] == "harmful" and r["label"] == "harmful")
    tn = sum(1 for r in best_data if r["expected"] == "safe" and r["label"] == "safe")
    fp = sum(1 for r in best_data if r["expected"] == "safe" and r["label"] == "harmful")
    fn = sum(1 for r in best_data if r["expected"] == "harmful" and r["label"] == "safe")

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    print(f"\nBest Method Details ({best_method}):")
    print(f"  Precision: {precision*100:.1f}%")
    print(f"  Recall: {recall*100:.1f}%")
    print(f"  F1: {f1*100:.1f}%")
    print(f"  TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}")


if __name__ == "__main__":
    main()
